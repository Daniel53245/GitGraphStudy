{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file will deal with json file obtained in data fetching and output them as csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic data \n",
    "In the below section you can assigne the destination of file and projects that you want to deal with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name of projects tat you have collected\n",
    "project_list = [\"numpy\",\"networkx\",\"OpenMetaData\",\"tensorflow\"]\n",
    "raw_json_paths = {}\n",
    "for project in project_list:\n",
    "    raw_json_paths[project] = f\"../Data/raw_json/{project}.json\"\n",
    "    print(f\"../Data/raw_json/{project}.json\")\n",
    "output_csv_path = {}\n",
    "for project in project_list:\n",
    "    output_csv_path[project] = f\"../Data/csv/{project}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_code = {}\n",
    "for i in range(len(project_list)):\n",
    "    project_code[project_list[i]] = str(i)+ '0' +str(i)\n",
    "print(project_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Author info\n",
    "This section will output all nnecessary information about the author and committers into author.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_commit_data(filename):\n",
    "    \"\"\"Load commit data from a JSON file.\"\"\"\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def extract_unique_users(commit_data):\n",
    "    \"\"\"Extract unique authors and committers from commit data, focusing on detailed GitHub user information.\"\"\"\n",
    "    authors = {}\n",
    "    committers = {}\n",
    "    commits = commit_data\n",
    "    for commit in commits:\n",
    "        author = commit.get('author')\n",
    "        committer = commit.get('committer')\n",
    "\n",
    "        if author:\n",
    "            authors[author['id']] = {\n",
    "                'login': author.get('login'),\n",
    "                'id': author.get('id'),\n",
    "                'url': author.get('html_url')\n",
    "            }\n",
    "\n",
    "        if committer:\n",
    "            committers[committer['id']] = {\n",
    "                'login': committer.get('login'),\n",
    "                'id': committer.get('id'),\n",
    "                'url': committer.get('html_url')\n",
    "            }\n",
    "\n",
    "    return list(authors.values()), list(committers.values())\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    \"\"\"Save detailed user data to a CSV file.\"\"\"\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        fieldnames = ['login', 'id', 'url']  # Define the headers of the CSV file\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for user in data:\n",
    "            writer.writerow(user)  # Write user data\n",
    "    print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for project in project_list:\n",
    "    input_json = raw_json_paths[project]# Ensure this path is correct\n",
    "    output_authors_csv = output_csv_path[project]+\"unique_authors.csv\"\n",
    "    output_committers_csv = output_csv_path[project]+\"unique_committers.csv\"\n",
    "    # Load commit data from JSON file\n",
    "    commit_data = load_commit_data(input_json)\n",
    "    print(f\"Loaded {len(commit_data)} Commits\")\n",
    "    print(commit_data[0].keys())\n",
    "    # Extract unique authors and committers\n",
    "    authors, committers = extract_unique_users(commit_data)\n",
    "\n",
    "    # Save the unique authors and committers to separate CSV files\n",
    "    save_to_csv(authors, output_authors_csv)\n",
    "    save_to_csv(committers, output_committers_csv)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit_data\n",
    "This section deals with commits, outcome will be saved to `commits.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_commits_to_csv(json_file, csv_file):\n",
    "    \"\"\"Save commit data from a JSON file to a CSV file with fixed headers.\n",
    "\n",
    "    Args:\n",
    "        json_file (str): Path to the input JSON file containing commit data.\n",
    "        csv_file (str): Path to the output CSV file to create.\n",
    "    \"\"\"\n",
    "    # Fixed CSV headers\n",
    "    csv_headers = ['self_sha', 'author_id', 'parent_sha', 'pr_id','date','committer_id']\n",
    "\n",
    "    # Load JSON data from the specified file\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        commits = json.load(file)\n",
    "    \n",
    "    # Open the CSV file for writing\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Write each commit to the CSV file\n",
    "        for commit in commits:\n",
    "            if commit['pull_requests']:\n",
    "                pr_id = commit['pull_requests'][0]['id']\n",
    "            else:\n",
    "                pr_id = -1  # Default PR ID if no pull request is associated\n",
    "            # check is auhtor null  \n",
    "            if commit['author']:\n",
    "                author_id = commit['author']['id']\n",
    "            else:\n",
    "                author_id = -1\n",
    "            writer.writerow({\n",
    "                'self_sha': commit['sha'],\n",
    "                'author_id': author_id,\n",
    "                'parent_sha': commit['parents'][0]['sha'],\n",
    "                'pr_id': pr_id,\n",
    "                'date': commit['commit']['author']['date'],\n",
    "                'committer_id':commit['committer']['id']\n",
    "            })\n",
    "    print(f'CSV file created successfully at {csv_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for project in project_list:\n",
    "    json_path = raw_json_paths[project]\n",
    "    csv_output = output_csv_path[project]+\"commits.csv\"\n",
    "    save_commits_to_csv(json_path,csv_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Request Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pull_requests(json_filepath, csv_filepath,project_code):\n",
    "    print(f\"Processing {csv_filepath}\")\n",
    "    # Load commits from JSON file\n",
    "    with open(json_filepath, 'r', encoding='utf-8') as file:\n",
    "        commits = json.load(file)\n",
    "    \n",
    "    pr_list = []\n",
    "    for commit in commits:\n",
    "        if 'pull_requests' in commit:\n",
    "            pr = commit['pull_requests']\n",
    "            pr_list.extend(pr)\n",
    "\n",
    "    # CSV headers\n",
    "    csv_headers = ['pr_id', 'author', 'head_sha', 'base_sha', 'created_at', 'merged_at', 'closed_at','reviewer_count']\n",
    "    id_set = set()\n",
    "\n",
    "    # Write to the CSV file\n",
    "    with open(csv_filepath, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "        writer.writeheader()\n",
    "        for pr in pr_list:\n",
    "            if pr['id'] in id_set:\n",
    "                continue\n",
    "            #print(pr['requested_reviewers'])\n",
    "            reviwer_count = len(pr['requested_reviewers'])\n",
    "            writer.writerow({\n",
    "                'pr_id': project_code + str(pr['id']),\n",
    "                'author': pr['user']['login'],\n",
    "                'head_sha': pr['head']['sha'],\n",
    "                'base_sha': pr['base']['sha'],\n",
    "                'created_at': pr['created_at'],\n",
    "                'closed_at': pr['closed_at'],\n",
    "                'merged_at': pr['merged_at'],\n",
    "                'reviewer_count':reviwer_count\n",
    "            })\n",
    "            id_set.add(pr['id'])  # Update the set to prevent duplicates\n",
    "\n",
    "    print(f\"Data written to {csv_filepath} successfully.\")\n",
    "\n",
    "# Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for project in project_list:\n",
    "    json_path = raw_json_paths[project]\n",
    "    csv_path = output_csv_path[project]+\"pull_requests.csv\"\n",
    "    process_pull_requests(json_path,csv_path,project_code[project])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting Reviewers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reviewers(json_filepath, csv_filepath,project_code):\n",
    "    print(f\"Processing {csv_filepath}\")\n",
    "    # Load commits from JSON file\n",
    "    with open(json_filepath, 'r', encoding='utf-8') as file:\n",
    "        commits = json.load(file)\n",
    "    \n",
    "    pr_list = []\n",
    "    for commit in commits:\n",
    "        if 'pull_requests' in commit:\n",
    "            pr = commit['pull_requests']\n",
    "            pr_list.extend(pr)\n",
    "\n",
    "    # CSV headers\n",
    "    csv_headers = ['pr_id','reviewer_id','reviewer_login']\n",
    "    id_set = set()\n",
    "\n",
    "    # Write to the CSV file\n",
    "    with open(csv_filepath, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=csv_headers)\n",
    "        writer.writeheader()\n",
    "        for pr in pr_list:\n",
    "            if pr['id'] in id_set:\n",
    "                continue\n",
    "            pr_id = project_code + str(pr['id'])\n",
    "            if not pr['requested_reviewers']:\n",
    "                continue\n",
    "            for reviewer in pr['requested_reviewers']:\n",
    "                writer.writerow({\n",
    "                    'pr_id':pr_id,\n",
    "                    'reviewer_id':reviewer['id'],\n",
    "                    'reviewer_login':reviewer['login']\n",
    "                })\n",
    "            id_set.add(pr['id'])  # Update the set to prevent duplicates\n",
    "\n",
    "    print(f\"Data written to {csv_filepath} successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge to single csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for project in project_list:\n",
    "    json_path = raw_json_paths[project]\n",
    "    csv_path = output_csv_path[project]+\"reviewer.csv\"\n",
    "    process_reviewers(json_path,csv_path,project_code[project])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV merging\n",
    "csv_name_list = ['commits.csv','pull_requests.csv','reviewer.csv','unique_authors.csv','unique_committers.csv']\n",
    "out_path_base = \"../Data/csv/Merged/\"\n",
    "for csv_name in csv_name_list:\n",
    "    dfs = []\n",
    "    for project in project_list:\n",
    "        csv_file = output_csv_path[project]+csv_name\n",
    "        if not os.path.exists(csv_file):\n",
    "            raise Exception(f\"csv_file:{csv_file} Not found, Aborting\") \n",
    "        df = pandas.read_csv(csv_file)\n",
    "        dfs.append(df)\n",
    "    merged_df = pandas.concat(dfs, ignore_index=True)\n",
    "    merged_df.to_csv(out_path_base+csv_name)\n",
    "    print(f\"File saved to {out_path_base+csv_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good no RSA colash\n",
    "out_path_base = \"../Data/csv/Merged/\"\n",
    "commit_df = pandas.read_csv(out_path_base+\"commits.csv\")\n",
    "if not len(commit_df) == len(commit_df['self_sha'].unique()):\n",
    "    raise Exception(\"RSA colash in prject\")\n",
    "else:\n",
    "    print(\"commit RSA all good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_request_df = pandas.read_csv('../Data/csv/Merged/pull_requests.csv')\n",
    "if not len(pull_request_df['pr_id'].unique())==len(pull_request_df):\n",
    "    print(\"There are duplicate PR_id\")\n",
    "else:\n",
    "    print(\"Pr_id all Good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df = pandas.read_csv('../Data/csv/Merged/unique_authors.csv')\n",
    "if not len(authors_df) == len(authors_df['id'].unique()):\n",
    "    print(\"There are duplicated entries in orginal file\")\n",
    "    duplicates = authors_df[authors_df.duplicated(subset='id', keep='first')]\n",
    "    df_unique = authors_df.drop_duplicates(subset='id',keep='first')\n",
    "    df_unique.to_csv('../Data/csv/Merged/authors.csv',index=False,header=True)\n",
    "else:\n",
    "    print(\"No duplicates\")\n",
    "authors_df = pandas.read_csv('../Data/csv/Merged/authors.csv')\n",
    "if not len(authors_df) == len(authors_df['id'].unique()):\n",
    "    print(\"There are duplicated entries\")\n",
    "    duplicates = authors_df[authors_df.duplicated(subset='id', keep='first')]\n",
    "    print(duplicates[:10])\n",
    "    raise Exception(\"Still duplication after dropping\")\n",
    "else:\n",
    "    print(\"File saved to author.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df = pandas.read_csv('../Data/csv/Merged/unique_committers.csv')\n",
    "if not len(authors_df) == len(authors_df['id'].unique()):\n",
    "    print(\"There are duplicated entries in orginal file\")\n",
    "    duplicates = authors_df[authors_df.duplicated(subset='id', keep='first')]\n",
    "    df_unique = authors_df.drop_duplicates(subset='id',keep='first')\n",
    "    df_unique.to_csv('../Data/csv/Merged/committers.csv',index=False,header=True)\n",
    "else:\n",
    "    print(\"No duplicates\")\n",
    "authors_df = pandas.read_csv('../Data/csv/Merged/committers.csv')\n",
    "if not len(authors_df) == len(authors_df['id'].unique()):\n",
    "    print(\"There are duplicated entries\")\n",
    "    duplicates = authors_df[authors_df.duplicated(subset='id', keep='first')]\n",
    "    print(duplicates[:10])\n",
    "    raise Exception(\"Still duplication after dropping\")\n",
    "else:\n",
    "    print(\"File saved to Committer.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
